# -*- coding: utf-8 -*-
"""Lab_01_190544E.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xm6bMkFnIcAlc6oNW3Csu3zLAML-mQjF
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.decomposition import PCA
import seaborn as sn
from imblearn.over_sampling import RandomOverSampler
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
import time
import os

from google.colab import drive
drive.mount('/content/drive')

import warnings
warnings.filterwarnings('ignore')

train = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ML lab 01/train.csv')
valid = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ML lab 01/valid.csv')
test = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/ML lab 01/test.csv')

X_train = train.iloc[:, :-4]
y_train = train.iloc[:, -4:]
X_val = valid.iloc[:, :-4]
y_val = valid.iloc[:, -4:]
X_test = test.iloc[:, :-4]
y_test = test.iloc[:, -4:]

X_train.head()
y_test.head()
X_test.head()

svm = SVC(kernel='linear')
def svm_classifier(X_train, Y_train, X_val, Y_val):
    svm.fit(X_train, Y_train)

    y_pred = svm.predict(X_val)

    accuracy = accuracy_score(Y_val, y_pred)
    return accuracy

knn = KNeighborsClassifier(n_neighbors=1)
def knn_classifier(X_train, Y_train, X_val, Y_val):

    knn.fit(np.array(X_train), Y_train)

    y_pred = knn.predict(np.array(X_val))

    accuracy = accuracy_score(Y_val, y_pred)
    return accuracy

logreg = LogisticRegression()
def logistic_regression_classifier(X_train, Y_train, X_val, Y_val):

    logreg.fit(X_train, Y_train)

    y_pred = logreg.predict(X_val)

    accuracy = accuracy_score(Y_val, y_pred)
    return accuracy

"""Label 1"""

plt.figure(figsize=(18, 6))
sn.countplot(data=y_train, x='label_1', palette='Set2')
plt.title('Distribution of label_1 Classes')
plt.xlabel('label_1')
plt.ylabel('Count')
plt.xticks(rotation=45)
plt.show()

# accuracy with all the features
#svm
start_time = time.time()
accuracy = svm_classifier(X_train, y_train['label_1'], X_val, y_val['label_1'] )
elapsed_time = time.time() - start_time
print(f"Accuracy: {accuracy * 100:.2f}% in {elapsed_time} secs")

# accuracy with all the features
#knn
start_time = time.time()
accuracy = knn_classifier(X_train, y_train['label_1'], X_val, y_val['label_1'] )
elapsed_time = time.time() - start_time
print(f"Accuracy: {accuracy * 100:.2f}% in {elapsed_time} secs")

# accuracy with all the features
#logistic_regression_classifier
start_time = time.time()
accuracy = logistic_regression_classifier(X_train, y_train['label_1'], X_val, y_val['label_1'] )
elapsed_time = time.time() - start_time
print(f"Accuracy: {accuracy * 100:.2f}% in {elapsed_time} secs")

label_1_pred_before = knn.predict(np.array(X_test))

def pca_feature_selection(dataset, label, threshold=0.95):
    X = dataset.drop(columns=[label])
    y = dataset[label]

    # Drop rows with NaN values
    X = X.dropna()

    pca = PCA()
    X_pca = pca.fit_transform(X)

    explained_variance_ratio = pca.explained_variance_ratio_
    cum_explained_variance_ratio = np.cumsum(explained_variance_ratio)

    n_components = np.argmax(cum_explained_variance_ratio >= threshold) + 1

    selected_features = X.columns[:n_components].tolist()

    return selected_features

pca_features = pca_feature_selection(train, 'label_1', 0.7)
len(set(pca_features))

pca_features = pca_feature_selection(train, 'label_1', 0.98)
print(f"dropping features count {len(set(pca_features))}")
X_train_filtered = X_train.drop(columns=list(pca_features))
X_val_filtered = X_val.drop(columns=list(pca_features))
X_test_filtered = X_test.drop(columns=list(pca_features))
start_time = time.time()
accuracy = svm_classifier(X_train_filtered, y_train['label_1'], X_val_filtered, y_val['label_1'] )
elapsed_time = time.time() - start_time
print(f"Accuracy: {accuracy * 100:.2f}% in {elapsed_time} secs")

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_filtered)
X_val_scaled = scaler.transform(X_val_filtered)
X_test_scaled = scaler.transform(X_test_filtered)

start_time = time.time()
accuracy = svm_classifier(X_train_scaled, y_train['label_1'], X_val_scaled, y_val['label_1'] )
elapsed_time = time.time() - start_time
print(f"Accuracy: {accuracy * 100:.2f}% in {elapsed_time} secs")

X_train_scaled.shape

pca = PCA(n_components=80, svd_solver = 'full')
X_train_pca = pca.fit_transform(X_train_scaled)
X_val_pca = pca.transform(X_val_scaled)
X_test_pca = pca.transform(X_test_scaled)

start_time = time.time()
accuracy = svm_classifier(X_train_pca, y_train['label_1'], X_val_pca, y_val['label_1'] )
elapsed_time = time.time() - start_time
print(f"Accuracy: {accuracy * 100:.2f}% in {elapsed_time} secs")

X_train_pca.shape

label_1_pred_after = svm.predict(np.array(X_test_pca))

label1_features = pd.DataFrame(data=X_test_pca, columns=[f'new_feature_{i+1}' for i in range(X_test_pca.shape[1])])
label1_features.insert(0,'Predicted labels before feature engineering',label_1_pred_before)
label1_features.insert(1,'Predicted labels after feature engineering', label_1_pred_after)
label1_features.insert(2,'No of new features', X_test_pca.shape[1])

"""Label 2"""

label2_train = train.copy()
label2_valid = valid.copy()
label2_test = test.copy()
label2_test.head()

label2_train = label2_train.dropna(subset=['label_2'])
label2_valid = label2_valid.dropna(subset=['label_2'])

X_train = label2_train.iloc[:, :-4]
y_train = label2_train.iloc[:, -3:]
X_val = label2_valid.iloc[:, :-4]
y_val = label2_valid.iloc[:, -3:]
X_test = label2_test.iloc[:, :-4]
y_test = label2_test.iloc[:, -3:]

X_test.head()

plt.figure(figsize=(18, 6))
sn.histplot(data=y_train, x='label_2', bins=20, kde=False)

# accuracy with all the features
#svm
start_time = time.time()
accuracy = svm_classifier(X_train, y_train['label_2'], X_val, y_val['label_2'] )
elapsed_time = time.time() - start_time
print(f"Accuracy: {accuracy * 100:.2f}% in {elapsed_time} secs")

# accuracy with all the features
#knn
start_time = time.time()
accuracy = knn_classifier(X_train, y_train['label_2'], X_val, y_val['label_2'] )
elapsed_time = time.time() - start_time
print(f"Accuracy: {accuracy * 100:.2f}% in {elapsed_time} secs")

# accuracy with all the features
#logistic_regression_classifier
start_time = time.time()
accuracy = logistic_regression_classifier(X_train, y_train['label_2'], X_val, y_val['label_2'] )
elapsed_time = time.time() - start_time
print(f"Accuracy: {accuracy * 100:.2f}% in {elapsed_time} secs")

label_2_pred_before = knn.predict(np.array(X_test))

pca_features = pca_feature_selection(train, 'label_2', 0.999)
print(f"dropping features count {len(set(pca_features))}")
X_train_filtered = X_train.drop(columns=list(pca_features))
X_val_filtered = X_val.drop(columns=list(pca_features))
X_test_filtered = X_test.drop(columns=list(pca_features))
start_time = time.time()
accuracy = knn_classifier(X_train_filtered, y_train['label_2'], X_val_filtered, y_val['label_2'] )
elapsed_time = time.time() - start_time
print(f"Accuracy: {accuracy * 100:.2f}% in {elapsed_time} secs")

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_filtered)
X_val_scaled = scaler.transform(X_val_filtered)
X_test_scaled = scaler.transform(X_test_filtered)

start_time = time.time()
accuracy = knn_classifier(X_train_scaled, y_train['label_2'], X_val_scaled, y_val['label_2'] )
elapsed_time = time.time() - start_time
print(f"Accuracy: {accuracy * 100:.2f}% in {elapsed_time} secs")

X_train_scaled.shape

pca = PCA(n_components=38, svd_solver = 'full')
X_train_pca = pca.fit_transform(X_train_scaled)
X_val_pca = pca.transform(X_val_scaled)
X_test_pca = pca.transform(X_test_scaled)

start_time = time.time()
accuracy = knn_classifier(X_train_pca, y_train['label_2'], X_val_pca, y_val['label_2'] )
elapsed_time = time.time() - start_time
print(f"Accuracy: {accuracy * 100:.2f}% in {elapsed_time} secs")

X_train_pca.shape

label_2_pred_after = knn.predict(np.array(X_test_pca))

label2_features = pd.DataFrame(data=X_test_pca, columns=[f'new_feature_{i+1}' for i in range(X_test_pca.shape[1])])
label2_features.insert(0,'Predicted labels before feature engineering',label_2_pred_before)
label2_features.insert(1,'Predicted labels after feature engineering', label_2_pred_after)
label2_features.insert(2,'No of new features', X_test_pca.shape[1])

"""Label 3"""

label3_train = train.copy()
label3_valid = valid.copy()
label3_test = test.copy()

X_train = label3_train.iloc[:, :-4]
y_train = label3_train.iloc[:, -2:]
X_val = label3_valid.iloc[:, :-4]
y_val = label3_valid.iloc[:, -2:]
X_test = label3_test.iloc[:, :-4]
y_test = label3_test.iloc[:, -2:]

ros = RandomOverSampler(random_state=0, sampling_strategy=0.75)
X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train['label_3'])

y_train_resampled

ax = sn.countplot(x=y_train['label_3'])

for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='bottom', fontsize=9, color='black')

# accuracy with all the features
#svm
start_time = time.time()
accuracy = svm_classifier(X_train_resampled, y_train_resampled, X_val, y_val['label_3'] )
elapsed_time = time.time() - start_time
print(f"Accuracy: {accuracy * 100:.2f}% in {elapsed_time} secs")

# accuracy with all the features
#knn
start_time = time.time()
accuracy = knn_classifier(X_train_resampled, y_train_resampled, X_val, y_val['label_3'] )
elapsed_time = time.time() - start_time
print(f"Accuracy: {accuracy * 100:.2f}% in {elapsed_time} secs")

# accuracy with all the features
#logistic_regression_classifier
start_time = time.time()
accuracy = logistic_regression_classifier(X_train_resampled, y_train_resampled, X_val, y_val['label_3'] )
elapsed_time = time.time() - start_time
print(f"Accuracy: {accuracy * 100:.2f}% in {elapsed_time} secs")

label_3_pred_before = knn.predict(np.array(X_test))

pca_features = pca_feature_selection(train, 'label_3', 0.99996)
print(f"dropping features count {len(set(pca_features))}")
X_train_filtered = X_train_resampled.drop(columns=list(pca_features))
X_val_filtered = X_val.drop(columns=list(pca_features))
X_test_filtered = X_test.drop(columns=list(pca_features))
start_time = time.time()
accuracy = logistic_regression_classifier(X_train_filtered, y_train_resampled, X_val_filtered, y_val['label_3'] )
elapsed_time = time.time() - start_time
print(f"Accuracy: {accuracy * 100:.2f}% in {elapsed_time} secs")

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_filtered)
X_val_scaled = scaler.transform(X_val_filtered)
X_test_scaled = scaler.transform(X_test_filtered)

X_train_scaled.shape

label_3_pred_after = logreg.predict(np.array(X_test_scaled))

label3_features = pd.DataFrame(data=X_test_pca, columns=[f'new_feature_{i+1}' for i in range(X_test_pca.shape[1])])
label3_features.insert(0,'Predicted labels before feature engineering',label_3_pred_before)
label3_features.insert(1,'Predicted labels after feature engineering', label_3_pred_after)
label3_features.insert(2,'No of new features', X_test_pca.shape[1])

"""Label 4"""

label4_train = train.copy()
label4_valid = valid.copy()
label4_test = test.copy()

X_train = label4_train.iloc[:, :-4]
y_train = label4_train.iloc[:, -1:]
X_val = label4_valid.iloc[:, :-4]
y_val = label4_valid.iloc[:, -1:]
X_test = label4_test.iloc[:, :-4]
y_test = label4_test.iloc[:, -1:]

plt.figure(figsize=(18, 6))
ax = sn.countplot(x=y_train['label_4'], color='teal')

for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='bottom', fontsize=9, color='black')

# accuracy with all the features
#knn
start_time = time.time()
accuracy = knn_classifier(X_train_resampled, y_train_resampled, X_val, y_val['label_4'] )
elapsed_time = time.time() - start_time
print(f"Accuracy: {accuracy * 100:.2f}% in {elapsed_time} secs")

label_4_pred_before = knn.predict(np.array(X_test))

ros = RandomOverSampler(random_state=0)
X_train_resampled, y_train_resampled = ros.fit_resample(X_train, y_train['label_4'])

plt.figure(figsize=(18, 6))
ax = sn.countplot(x=y_train_resampled, color='teal')

for p in ax.patches:
    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),
                ha='center', va='bottom', fontsize=9, color='black')

# accuracy with all the features
#svm
start_time = time.time()
accuracy = svm_classifier(X_train_resampled, y_train_resampled, X_val, y_val['label_4'] )
elapsed_time = time.time() - start_time
print(f"Accuracy: {accuracy * 100:.2f}% in {elapsed_time} secs")

# accuracy with all the features
#knn
start_time = time.time()
accuracy = knn_classifier(X_train_resampled, y_train_resampled, X_val, y_val['label_4'] )
elapsed_time = time.time() - start_time
print(f"Accuracy: {accuracy * 100:.2f}% in {elapsed_time} secs")

# accuracy with all the features
#logistic_regression_classifier
start_time = time.time()
accuracy = logistic_regression_classifier(X_train_resampled, y_train_resampled, X_val, y_val['label_4'] )
elapsed_time = time.time() - start_time
print(f"Accuracy: {accuracy * 100:.2f}% in {elapsed_time} secs")

pca_features = pca_feature_selection(train, 'label_4', 0.9999)
print(f"dropping features count {len(set(pca_features))}")
X_train_filtered = X_train_resampled.drop(columns=list(pca_features))
X_val_filtered = X_val.drop(columns=list(pca_features))
X_test_filtered = X_test.drop(columns=list(pca_features))
start_time = time.time()
accuracy = knn_classifier(X_train_filtered, y_train_resampled, X_val_filtered, y_val['label_4'] )
elapsed_time = time.time() - start_time
print(f"Accuracy: {accuracy * 100:.2f}% in {elapsed_time} secs")

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_filtered)
X_val_scaled = scaler.transform(X_val_filtered)
X_test_scaled = scaler.transform(X_test_filtered)

X_train_scaled.shape

pca = PCA(n_components=36, svd_solver = 'full')
X_train_pca = pca.fit_transform(X_train_scaled)
X_val_pca = pca.transform(X_val_scaled)
X_test_pca = pca.transform(X_test_scaled)

X_train_pca.shape

start_time = time.time()
accuracy = knn_classifier(X_train_pca, y_train_resampled, X_val_pca, y_val['label_4'] )
elapsed_time = time.time() - start_time
print(f"Accuracy: {accuracy * 100:.2f}% in {elapsed_time} secs")

"""Save Output to CSV"""

label_4_pred_after = knn.predict(np.array(X_test_pca))

label4_features = pd.DataFrame(data=X_test_pca, columns=[f'new_feature_{i+1}' for i in range(X_test_pca.shape[1])])
label4_features.insert(0,'Predicted labels before feature engineering',label_4_pred_before)
label4_features.insert(1,'Predicted labels after feature engineering', label_4_pred_after)
label4_features.insert(2,'No of new features', X_test_pca.shape[1])

def write_csv(feature_df, label):
  dir_name = 'output'
  if not os.path.exists(dir_name):
    os.makedirs(dir_name)
  for i in range(feature_df['No of new features'][0], 256):
        feature_df[f'new_feature_{i+1}'] = [''] * (feature_df.shape[0])
  filename = f'output/190544E_label_{label}.csv'
  feature_df.to_csv(filename, index=False)

write_csv(label1_features.copy(), 1)
write_csv(label2_features.copy(), 2)
write_csv(label3_features.copy(), 3)
write_csv(label4_features.copy(), 4)